# -*- coding: utf-8 -*-
"""
Fraud Detection with Gaussian Bayesian Networks and Gaussian Processes

This implementation explores continuous-data approaches:
1. Gaussian Bayesian Networks (GBN) - assumes continuous features follow Gaussian distribution
2. Gaussian Processes (GP) - non-parametric Bayesian approach for classification

Both methods work with raw continuous features (no discretization required).
"""

import pandas as pd
import numpy as np
from pathlib import Path
from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.metrics import (accuracy_score, precision_score, recall_score, 
                            f1_score, roc_auc_score, confusion_matrix, brier_score_loss)
from sklearn.preprocessing import StandardScaler, LabelEncoder
import time
import warnings
warnings.filterwarnings('ignore')

# Gaussian Bayesian Networks (pgmpy)
try:
    from pgmpy.models import LinearGaussianBayesianNetwork
    from pgmpy.estimators import HillClimbSearch, BIC
    PGMPY_AVAILABLE = True
except Exception as e:
    PGMPY_AVAILABLE = False
    print(f"‚ö†Ô∏è  pgmpy not fully available for Gaussian BN: {e}")

# Gaussian Processes (GPyTorch)
try:
    import torch
    import gpytorch
    from gpytorch.models import ApproximateGP
    from gpytorch.variational import CholeskyVariationalDistribution, VariationalStrategy
    GPYTORCH_AVAILABLE = True
except Exception as e:
    GPYTORCH_AVAILABLE = False
    print(f"‚ö†Ô∏è  gpytorch not available: {e}")
    print("   Install with: pip install gpytorch")


def load_and_prepare_fraud_data(csv_path, sample_size=5000):
    """
    Load fraud dataset and prepare for Gaussian models.
    
    Args:
        csv_path: Path to synthetic_fraud_dataset.csv
        sample_size: Subsample for GP (GP scales O(n¬≥), expensive for large datasets)
    
    Returns:
        X: Continuous features (Transaction_Amount + encoded categoricals)
        y: Binary fraud labels
        feature_names: List of feature names
    """
    print(f"\nLoading {csv_path}...")
    df = pd.read_csv(csv_path)
    
    print(f"Dataset: {len(df):,} transactions")
    print(f"Fraud rate: {df['Fraud_Label'].mean():.2%}")
    
    # Separate continuous and categorical features
    target = 'Fraud_Label'
    continuous_features = ['Transaction_Amount']
    categorical_features = [col for col in df.columns 
                           if col not in continuous_features + [target]]
    
    print(f"\nFeature breakdown:")
    print(f"  Continuous: {len(continuous_features)} features")
    print(f"  Categorical: {len(categorical_features)} features")
    
    # Encode categorical features as numeric (label encoding for GBN)
    df_processed = df.copy()
    label_encoders = {}
    
    for col in categorical_features:
        le = LabelEncoder()
        df_processed[col] = le.fit_transform(df[col].astype(str))
        label_encoders[col] = le
    
    # Prepare feature matrix and target
    feature_cols = continuous_features + categorical_features
    X = df_processed[feature_cols].values.astype(float)
    y = df_processed[target].values
    
    # Subsample for computational efficiency (especially for GP)
    if len(df) > sample_size:
        print(f"\n‚ö†Ô∏è  Subsampling to {sample_size:,} transactions for efficiency")
        print(f"   (Gaussian Processes scale O(n¬≥), expensive for large datasets)")
        indices = np.random.choice(len(X), sample_size, replace=False)
        X = X[indices]
        y = y[indices]
    
    print(f"\n‚úì Prepared {X.shape[0]:,} samples with {X.shape[1]} features")
    print(f"  Fraud rate in sample: {y.mean():.2%}")
    
    return X, y, feature_cols


# ==================== GAUSSIAN BAYESIAN NETWORK ====================

def build_gaussian_bayesian_network(X_train, y_train, feature_names):
    """
    Build Gaussian Bayesian Network using pgmpy.
    
    GBN assumes linear Gaussian relationships: X_i = Œ≤‚ÇÄ + Œ£(Œ≤_j * X_parent_j) + Œµ
    where Œµ ~ N(0, œÉ¬≤)
    
    Args:
        X_train: Training features
        y_train: Training labels
        feature_names: Feature column names
    
    Returns:
        model: Fitted LinearGaussianBayesianNetwork
        learning_time: Time taken to learn structure + parameters
    """
    if not PGMPY_AVAILABLE:
        print("\n‚ùå pgmpy not available for Gaussian Bayesian Networks")
        return None, 0
    
    print("\n" + "="*70)
    print("GAUSSIAN BAYESIAN NETWORK")
    print("="*70)
    
    # Create DataFrame with feature names
    df_train = pd.DataFrame(X_train, columns=feature_names)
    df_train['Fraud_Label'] = y_train
    
    print("\nLearning structure with Hill Climbing + BIC...")
    print("(GBN assumes linear Gaussian relationships between variables)")
    
    try:
        start_time = time.time()
        
        # Structure learning
        hc = HillClimbSearch(df_train)
        best_model = hc.estimate(scoring_method=BIC(df_train))
        
        structure_time = time.time() - start_time
        print(f"\n‚úì Structure learned in {structure_time:.2f}s")
        print(f"  Total edges: {len(best_model.edges())}")
        
        target_edges = [edge for edge in best_model.edges() if 'Fraud_Label' in edge]
        print(f"  Fraud-connected edges: {len(target_edges)}")
        
        if target_edges:
            print(f"\n  Predictive edges:")
            for edge in target_edges:
                print(f"    {edge[0]} ‚Üí {edge[1]}")
        
        # Parameter learning for Linear Gaussian BN
        print("\nLearning parameters (linear Gaussian CPDs)...")
        model = LinearGaussianBayesianNetwork(best_model.edges())
        model.fit(df_train)
        
        learning_time = time.time() - start_time
        print(f"‚úì Total learning time: {learning_time:.2f}s")
        
        # Show a sample CPD
        if 'Fraud_Label' in model.nodes():
            cpd = model.get_cpds('Fraud_Label')
            if cpd and hasattr(cpd, 'mean'):
                print(f"\n  Fraud_Label CPD:")
                print(f"    Mean: {cpd.mean:.4f}")
                if hasattr(cpd, 'variance'):
                    print(f"    Variance: {cpd.variance:.4f}")
        
        return model, learning_time
        
    except Exception as e:
        print(f"\n‚ùå Gaussian BN failed: {e}")
        print("   Note: GBN requires continuous data with Gaussian distributions")
        return None, 0


def evaluate_gaussian_bn(model, X_test, y_test, feature_names):
    """
    Evaluate Gaussian Bayesian Network on test set.
    
    For Linear Gaussian BN, prediction is challenging because:
    - GBN models joint Gaussian distribution
    - Standard inference gives continuous predictions
    - Need to convert to binary classification
    
    Args:
        model: Fitted LinearGaussianBayesianNetwork
        X_test: Test features
        y_test: True labels
        feature_names: Feature names
    
    Returns:
        metrics: Dictionary of performance metrics
    """
    if model is None:
        return None
    
    print("\n" + "="*70)
    print("EVALUATING GAUSSIAN BAYESIAN NETWORK")
    print("="*70)
    
    try:
        df_test = pd.DataFrame(X_test, columns=feature_names)
        
        # For Linear Gaussian BN, we can:
        # 1. Predict the mean value of Fraud_Label given evidence
        # 2. Threshold at 0.5 for classification
        
        print("\nPredicting fraud probabilities...")
        predictions = []
        
        # Predict using the learned Gaussian distributions
        # For simplicity, we'll use a heuristic: predict based on weighted features
        # In a true GBN, we'd use inference to get P(Fraud_Label | evidence)
        
        # Get CPD for Fraud_Label
        if 'Fraud_Label' in model.nodes():
            cpd = model.get_cpds('Fraud_Label')
            
            # Simple prediction: use learned mean and parents
            parents = list(model.predecessors('Fraud_Label'))
            
            if parents and hasattr(cpd, 'mean') and hasattr(cpd, 'beta_vector'):
                # Linear prediction: E[Fraud_Label] = mean + Œ£(beta_i * parent_i)
                beta = cpd.beta_vector
                mean = cpd.mean
                
                for i in range(len(X_test)):
                    # Get parent values
                    evidence_values = []
                    for parent in parents:
                        parent_idx = feature_names.index(parent)
                        evidence_values.append(X_test[i, parent_idx])
                    
                    # Linear prediction
                    prediction = mean + np.dot(beta, evidence_values)
                    predictions.append(prediction)
                
                predictions = np.array(predictions)
                
                # Normalize to [0, 1] range for probability interpretation
                predictions_normalized = (predictions - predictions.min()) / (predictions.max() - predictions.min() + 1e-10)
                
            else:
                # Fallback: use mean prediction
                print("  ‚ö†Ô∏è  Using mean prediction (no parents found)")
                predictions_normalized = np.full(len(X_test), cpd.mean if hasattr(cpd, 'mean') else 0.5)
        else:
            print("  ‚ö†Ô∏è  Fraud_Label not in model, using random predictions")
            predictions_normalized = np.random.uniform(0, 1, len(X_test))
        
        # Convert to binary predictions
        y_pred = (predictions_normalized >= 0.5).astype(int)
        
        # Calculate metrics
        accuracy = accuracy_score(y_test, y_pred)
        precision = precision_score(y_test, y_pred, zero_division=0)
        recall = recall_score(y_test, y_pred, zero_division=0)
        f1 = f1_score(y_test, y_pred, zero_division=0)
        
        try:
            roc_auc = roc_auc_score(y_test, predictions_normalized)
        except:
            roc_auc = 0.5
        
        try:
            brier = brier_score_loss(y_test, predictions_normalized)
        except:
            brier = 0.25
        
        cm = confusion_matrix(y_test, y_pred)
        
        print(f"\n‚úì Predictions completed")
        print(f"  Probability range: [{predictions_normalized.min():.4f}, {predictions_normalized.max():.4f}]")
        print(f"  Mean probability: {predictions_normalized.mean():.4f}")
        
        print(f"\nGaussian Bayesian Network Performance:")
        print(f"{'='*50}")
        print(f"  Accuracy:        {accuracy:.4f}")
        print(f"  Precision:       {precision:.4f}")
        print(f"  Recall:          {recall:.4f}")
        print(f"  F1-Score:        {f1:.4f}")
        print(f"  ROC AUC:         {roc_auc:.4f}")
        print(f"  Brier Score:     {brier:.4f}")
        
        print(f"\n  Confusion Matrix:")
        print(f"            Predicted")
        print(f"          0        1")
        print(f"  Actual 0 {cm[0,0]:5d}   {cm[0,1]:5d}")
        print(f"         1 {cm[1,0]:5d}   {cm[1,1]:5d}")
        
        return {
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1': f1,
            'roc_auc': roc_auc,
            'brier_score': brier,
            'confusion_matrix': cm,
            'predictions': predictions_normalized
        }
        
    except Exception as e:
        print(f"\n‚ùå Evaluation failed: {e}")
        return None


# ==================== GAUSSIAN PROCESS CLASSIFIER ====================

class GaussianProcessClassifier(ApproximateGP):
    """
    Gaussian Process for binary classification using GPyTorch.
    
    Uses variational inference with inducing points for scalability.
    Suitable for datasets up to ~10K samples.
    
    Architecture:
    - RBF kernel (models smooth functions)
    - Variational inference with inducing points
    - Bernoulli likelihood for binary classification
    """
    
    def __init__(self, inducing_points):
        """
        Initialize GP classifier.
        
        Args:
            inducing_points: Inducing point locations for variational inference
        """
        variational_distribution = CholeskyVariationalDistribution(inducing_points.size(0))
        variational_strategy = VariationalStrategy(
            self, inducing_points, variational_distribution, learn_inducing_locations=True
        )
        super(GaussianProcessClassifier, self).__init__(variational_strategy)
        
        self.mean_module = gpytorch.means.ConstantMean()
        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())
    
    def forward(self, x):
        """
        Forward pass through GP.
        
        Args:
            x: Input features
        
        Returns:
            Multivariate normal distribution
        """
        mean_x = self.mean_module(x)
        covar_x = self.covar_module(x)
        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)


def train_gaussian_process(X_train, y_train, n_inducing=500, n_epochs=50):
    """
    Train Gaussian Process classifier.
    
    Args:
        X_train: Training features (numpy array)
        y_train: Training labels (numpy array)
        n_inducing: Number of inducing points for variational inference
        n_epochs: Training epochs
    
    Returns:
        model: Trained GP model
        likelihood: Bernoulli likelihood
        training_time: Time taken to train
    """
    if not GPYTORCH_AVAILABLE:
        print("\n‚ùå gpytorch not available for Gaussian Processes")
        print("   Install with: pip install gpytorch")
        return None, None, 0
    
    print("\n" + "="*70)
    print("GAUSSIAN PROCESS CLASSIFIER")
    print("="*70)
    
    print(f"\nTraining GP with variational inference...")
    print(f"  Training samples: {len(X_train):,}")
    print(f"  Inducing points: {n_inducing}")
    print(f"  Epochs: {n_epochs}")
    print(f"  Kernel: RBF (Radial Basis Function)")
    
    try:
        start_time = time.time()
        
        # Convert to PyTorch tensors
        X_train_tensor = torch.tensor(X_train, dtype=torch.float32)
        y_train_tensor = torch.tensor(y_train, dtype=torch.float32)
        
        # Standardize features
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)
        
        # Initialize inducing points (subsample of training data)
        n_inducing = min(n_inducing, len(X_train))
        inducing_indices = np.random.choice(len(X_train), n_inducing, replace=False)
        inducing_points = X_train_tensor[inducing_indices]
        
        # Initialize model and likelihood
        model = GaussianProcessClassifier(inducing_points=inducing_points)
        likelihood = gpytorch.likelihoods.BernoulliLikelihood()
        
        # Training mode
        model.train()
        likelihood.train()
        
        # Optimizer
        optimizer = torch.optim.Adam([
            {'params': model.parameters()},
            {'params': likelihood.parameters()}
        ], lr=0.01)
        
        # Loss function (marginal log likelihood)
        mll = gpytorch.mlls.VariationalELBO(likelihood, model, num_data=len(y_train_tensor))
        
        # Training loop
        print("\nTraining progress:")
        for epoch in range(n_epochs):
            optimizer.zero_grad()
            output = model(X_train_tensor)
            loss = -mll(output, y_train_tensor)
            loss.backward()
            optimizer.step()
            
            if (epoch + 1) % 10 == 0:
                print(f"  Epoch {epoch+1:3d}/{n_epochs}: Loss = {loss.item():.4f}")
        
        training_time = time.time() - start_time
        print(f"\n‚úì Training completed in {training_time:.2f}s")
        
        # Store scaler for test predictions
        model.scaler = scaler
        
        return model, likelihood, training_time
        
    except Exception as e:
        print(f"\n‚ùå GP training failed: {e}")
        import traceback
        traceback.print_exc()
        return None, None, 0


def evaluate_gaussian_process(model, likelihood, X_test, y_test):
    """
    Evaluate Gaussian Process classifier.
    
    Args:
        model: Trained GP model
        likelihood: Bernoulli likelihood
        X_test: Test features
        y_test: True labels
    
    Returns:
        metrics: Dictionary of performance metrics
    """
    if model is None or likelihood is None:
        return None
    
    print("\n" + "="*70)
    print("EVALUATING GAUSSIAN PROCESS")
    print("="*70)
    
    try:
        # Convert to tensors and standardize
        X_test_scaled = model.scaler.transform(X_test)
        X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)
        
        # Evaluation mode
        model.eval()
        likelihood.eval()
        
        print("\nMaking predictions...")
        
        # Make predictions
        with torch.no_grad(), gpytorch.settings.fast_pred_var():
            observed_pred = likelihood(model(X_test_tensor))
            pred_probs = observed_pred.mean.numpy()
        
        # Use optimal threshold based on training data distribution
        # (instead of fixed 0.5, use the fraud rate as threshold)
        optimal_threshold = y_test.mean()
        y_pred = (pred_probs >= optimal_threshold).astype(int)
        
        # Calculate metrics
        accuracy = accuracy_score(y_test, y_pred)
        precision = precision_score(y_test, y_pred, zero_division=0)
        recall = recall_score(y_test, y_pred, zero_division=0)
        f1 = f1_score(y_test, y_pred, zero_division=0)
        
        try:
            roc_auc = roc_auc_score(y_test, pred_probs)
        except:
            roc_auc = 0.5
        
        try:
            brier = brier_score_loss(y_test, pred_probs)
        except:
            brier = 0.25
        
        cm = confusion_matrix(y_test, y_pred)
        
        print(f"\n‚úì Predictions completed")
        print(f"  Probability range: [{pred_probs.min():.4f}, {pred_probs.max():.4f}]")
        print(f"  Mean probability: {pred_probs.mean():.4f}")
        print(f"  Optimal threshold: {optimal_threshold:.4f} (based on class distribution)")
        
        print(f"\nGaussian Process Performance:")
        print(f"{'='*50}")
        print(f"  Accuracy:        {accuracy:.4f}")
        print(f"  Precision:       {precision:.4f}")
        print(f"  Recall:          {recall:.4f}")
        print(f"  F1-Score:        {f1:.4f}")
        print(f"  ROC AUC:         {roc_auc:.4f}")
        print(f"  Brier Score:     {brier:.4f}")
        
        print(f"\n  Confusion Matrix:")
        print(f"            Predicted")
        print(f"          0        1")
        print(f"  Actual 0 {cm[0,0]:5d}   {cm[0,1]:5d}")
        print(f"         1 {cm[1,0]:5d}   {cm[1,1]:5d}")
        
        return {
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1': f1,
            'roc_auc': roc_auc,
            'brier_score': brier,
            'confusion_matrix': cm,
            'predictions': pred_probs
        }
        
    except Exception as e:
        print(f"\n‚ùå Evaluation failed: {e}")
        import traceback
        traceback.print_exc()
        return None


# ==================== MAIN EXECUTION ====================

def main():
    """
    Main execution: Compare Gaussian BN and Gaussian Process on fraud detection.
    """
    print("="*70)
    print("FRAUD DETECTION: GAUSSIAN BAYESIAN NETWORKS vs GAUSSIAN PROCESSES")
    print("="*70)
    
    print("\nThis implementation explores continuous-data Bayesian approaches:")
    print("  1. Gaussian Bayesian Networks (Linear Gaussian CPDs)")
    print("  2. Gaussian Processes (Non-parametric Bayesian classification)")
    
    # Load data
    csv_path = Path("synthetic_fraud_dataset.csv")
    if not csv_path.exists():
        print(f"\n‚ùå Dataset not found: {csv_path}")
        print("   Please ensure synthetic_fraud_dataset.csv is in the current directory.")
        return
    
    # Use subset for GP efficiency
    X, y, feature_names = load_and_prepare_fraud_data(csv_path, sample_size=5000)
    
    # Train-test split (80-20)
    print("\n" + "="*70)
    print("TRAIN-TEST SPLIT")
    print("="*70)
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )
    print(f"\nTraining set: {len(X_train):,} samples")
    print(f"Test set:     {len(X_test):,} samples")
    print(f"Train fraud rate: {y_train.mean():.2%}")
    print(f"Test fraud rate:  {y_test.mean():.2%}")
    
    # ==================== Gaussian Bayesian Network ====================
    
    gbn_model, gbn_time = build_gaussian_bayesian_network(X_train, y_train, feature_names)
    gbn_metrics = evaluate_gaussian_bn(gbn_model, X_test, y_test, feature_names) if gbn_model else None
    
    # ==================== Gaussian Process ====================
    
    gp_model, gp_likelihood, gp_time = train_gaussian_process(
        X_train, y_train, n_inducing=500, n_epochs=50
    )
    gp_metrics = evaluate_gaussian_process(gp_model, gp_likelihood, X_test, y_test) if gp_model else None
    
    # ==================== Final Comparison ====================
    
    print("\n" + "="*70)
    print("FINAL COMPARISON")
    print("="*70)
    
    print("\n" + "‚îÄ"*70)
    print("METHOD COMPARISON")
    print("‚îÄ"*70)
    
    print("\n1. Gaussian Bayesian Network:")
    print("   ‚Ä¢ Assumes linear Gaussian relationships between variables")
    print("   ‚Ä¢ Structure learning with Hill Climbing + BIC")
    print("   ‚Ä¢ Fast inference once structure is learned")
    print("   ‚Ä¢ Interpretable (shows feature dependencies)")
    if gbn_metrics:
        print(f"   ‚Ä¢ ROC AUC: {gbn_metrics['roc_auc']:.4f}")
        print(f"   ‚Ä¢ Accuracy: {gbn_metrics['accuracy']:.4f}")
        print(f"   ‚Ä¢ Training time: {gbn_time:.2f}s")
    else:
        print("   ‚Ä¢ Status: Failed (requires continuous data)")
    
    print("\n2. Gaussian Process:")
    print("   ‚Ä¢ Non-parametric Bayesian approach")
    print("   ‚Ä¢ RBF kernel for smooth decision boundaries")
    print("   ‚Ä¢ Provides uncertainty estimates")
    print("   ‚Ä¢ Flexible but computationally expensive (O(n¬≥))")
    if gp_metrics:
        print(f"   ‚Ä¢ ROC AUC: {gp_metrics['roc_auc']:.4f}")
        print(f"   ‚Ä¢ Accuracy: {gp_metrics['accuracy']:.4f}")
        print(f"   ‚Ä¢ Training time: {gp_time:.2f}s")
    else:
        print("   ‚Ä¢ Status: Failed (gpytorch not available)")
    
    print("\n" + "‚îÄ"*70)
    print("KEY INSIGHTS")
    print("‚îÄ"*70)
    
    print("\n‚ö†Ô∏è  Expected Performance:")
    print("   Both Gaussian models will struggle with this dataset because:")
    print("   ‚Ä¢ Fraud data has uniform random distributions (Cram√©r's V < 0.014)")
    print("   ‚Ä¢ No real patterns to learn (confirmed by discrete BN analysis)")
    print("   ‚Ä¢ Categorical features are label-encoded (loses categorical nature)")
    
    print("\n‚úì Methodology Demonstration:")
    print("   This implementation shows:")
    print("   ‚Ä¢ Gaussian BN for continuous relationships")
    print("   ‚Ä¢ Gaussian Process for flexible Bayesian classification")
    print("   ‚Ä¢ Both approaches work on continuous features")
    print("   ‚Ä¢ Comparison with discrete BN from main pipeline")
    
    print("\nüìä Comparison with Discrete Bayesian Network:")
    print("   ‚Ä¢ Discrete BN (TreeSearch TAN): ROC AUC 0.5007")
    print("   ‚Ä¢ All approaches ~50% accuracy (dataset limitation)")
    print("   ‚Ä¢ Heart disease dataset achieved 95% accuracy (proves BN works!)")
    
    print("\n" + "="*70)
    print("ANALYSIS COMPLETE")
    print("="*70)


if __name__ == "__main__":
    main()
